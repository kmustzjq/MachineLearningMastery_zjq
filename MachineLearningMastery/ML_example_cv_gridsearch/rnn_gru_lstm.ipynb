{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#print(torch.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 10]) torch.Size([2, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "#输入特征维度5，输出维度10, 层数2\n",
    "rnn = torch.nn.RNN(5, 10, 2)\n",
    "#rnn = torch.nn.RNN(5, 10, 2,bidirectional=True)\n",
    "#seq长度4，batch_size=2\n",
    "input = torch.randn(4 , 2 , 5)\n",
    "h_0 =torch.randn(2 , 2 , 10)\n",
    "output0,hn0=rnn(input ,h_0) \n",
    "\n",
    "print(output0.size(),hn0.size())\n",
    "#>>torch.Size([4, 2, 10]) torch.Size([2, 2, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight_ih_l0', torch.Size([10, 5])),\n",
       " ('weight_hh_l0', torch.Size([10, 10])),\n",
       " ('bias_ih_l0', torch.Size([10])),\n",
       " ('bias_hh_l0', torch.Size([10])),\n",
       " ('weight_ih_l1', torch.Size([10, 10])),\n",
       " ('weight_hh_l1', torch.Size([10, 10])),\n",
       " ('bias_ih_l1', torch.Size([10])),\n",
       " ('bias_hh_l1', torch.Size([10]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(para[0],para[1].shape) for para in list(rnn.named_parameters())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight_ih_l0', torch.Size([10, 5])),\n",
       " ('weight_hh_l0', torch.Size([10, 10])),\n",
       " ('bias_ih_l0', torch.Size([10])),\n",
       " ('bias_hh_l0', torch.Size([10])),\n",
       " ('weight_ih_l0_reverse', torch.Size([10, 5])),\n",
       " ('weight_hh_l0_reverse', torch.Size([10, 10])),\n",
       " ('bias_ih_l0_reverse', torch.Size([10])),\n",
       " ('bias_hh_l0_reverse', torch.Size([10])),\n",
       " ('weight_ih_l1', torch.Size([10, 20])),\n",
       " ('weight_hh_l1', torch.Size([10, 10])),\n",
       " ('bias_ih_l1', torch.Size([10])),\n",
       " ('bias_hh_l1', torch.Size([10])),\n",
       " ('weight_ih_l1_reverse', torch.Size([10, 20])),\n",
       " ('weight_hh_l1_reverse', torch.Size([10, 10])),\n",
       " ('bias_ih_l1_reverse', torch.Size([10])),\n",
       " ('bias_hh_l1_reverse', torch.Size([10]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = torch.nn.RNN(5, 10, 2,bidirectional=True)\n",
    "[(para[0],para[1].shape) for para in list(rnn.named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n",
       "\n",
       "\n",
       "For each element in the input sequence, each layer computes the following\n",
       "function:\n",
       "\n",
       ".. math::\n",
       "    \\begin{array}{ll}\n",
       "        r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
       "        z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
       "        n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
       "        h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}\n",
       "    \\end{array}\n",
       "\n",
       "where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the input\n",
       "at time `t`, :math:`h_{(t-1)}` is the hidden state of the layer\n",
       "at time `t-1` or the initial hidden state at time `0`, and :math:`r_t`,\n",
       ":math:`z_t`, :math:`n_t` are the reset, update, and new gates, respectively.\n",
       ":math:`\\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.\n",
       "\n",
       "In a multilayer GRU, the input :math:`x^{(l)}_t` of the :math:`l` -th layer\n",
       "(:math:`l >= 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by\n",
       "dropout :math:`\\delta^{(l-1)}_t` where each :math:`\\delta^{(l-1)}_t` is a Bernoulli random\n",
       "variable which is :math:`0` with probability :attr:`dropout`.\n",
       "\n",
       "Args:\n",
       "    input_size: The number of expected features in the input `x`\n",
       "    hidden_size: The number of features in the hidden state `h`\n",
       "    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
       "        would mean stacking two GRUs together to form a `stacked GRU`,\n",
       "        with the second GRU taking in outputs of the first GRU and\n",
       "        computing the final results. Default: 1\n",
       "    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
       "        Default: ``True``\n",
       "    batch_first: If ``True``, then the input and output tensors are provided\n",
       "        as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n",
       "        Note that this does not apply to hidden or cell states. See the\n",
       "        Inputs/Outputs sections below for details.  Default: ``False``\n",
       "    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
       "        GRU layer except the last layer, with dropout probability equal to\n",
       "        :attr:`dropout`. Default: 0\n",
       "    bidirectional: If ``True``, becomes a bidirectional GRU. Default: ``False``\n",
       "\n",
       "Inputs: input, h_0\n",
       "    * **input**: tensor of shape :math:`(L, N, H_{in})` when ``batch_first=False`` or\n",
       "      :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n",
       "      the input sequence.  The input can also be a packed variable length sequence.\n",
       "      See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
       "      :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
       "    * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the initial hidden\n",
       "      state for each element in the batch. Defaults to zeros if not provided.\n",
       "\n",
       "    where:\n",
       "\n",
       "    .. math::\n",
       "        \\begin{aligned}\n",
       "            N ={} & \\text{batch size} \\\\\n",
       "            L ={} & \\text{sequence length} \\\\\n",
       "            D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n",
       "            H_{in} ={} & \\text{input\\_size} \\\\\n",
       "            H_{out} ={} & \\text{hidden\\_size}\n",
       "        \\end{aligned}\n",
       "\n",
       "Outputs: output, h_n\n",
       "    * **output**: tensor of shape :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n",
       "      :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n",
       "      `(h_t)` from the last layer of the GRU, for each `t`. If a\n",
       "      :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n",
       "      will also be a packed sequence.\n",
       "    * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the final hidden state\n",
       "      for each element in the batch.\n",
       "\n",
       "Attributes:\n",
       "    weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n",
       "        (W_ir|W_iz|W_in), of shape `(3*hidden_size, input_size)` for `k = 0`.\n",
       "        Otherwise, the shape is `(3*hidden_size, num_directions * hidden_size)`\n",
       "    weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n",
       "        (W_hr|W_hz|W_hn), of shape `(3*hidden_size, hidden_size)`\n",
       "    bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n",
       "        (b_ir|b_iz|b_in), of shape `(3*hidden_size)`\n",
       "    bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n",
       "        (b_hr|b_hz|b_hn), of shape `(3*hidden_size)`\n",
       "\n",
       ".. note::\n",
       "    All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
       "    where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
       "\n",
       ".. note::\n",
       "    For bidirectional GRUs, forward and backward are directions 0 and 1 respectively.\n",
       "    Example of splitting the output layers when ``batch_first=False``:\n",
       "    ``output.view(seq_len, batch, num_directions, hidden_size)``.\n",
       "\n",
       ".. include:: ../cudnn_persistent_rnn.rst\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> rnn = nn.GRU(10, 20, 2)\n",
       "    >>> input = torch.randn(5, 3, 10)\n",
       "    >>> h0 = torch.randn(2, 3, 20)\n",
       "    >>> output, hn = rnn(input, h0)\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\admin\\appdata\\roaming\\python\\python36\\site-packages\\torch\\nn\\modules\\rnn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?torch.nn.GRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies a multi-layer long short-term memory (LSTM) RNN to an input\n",
       "sequence.\n",
       "\n",
       "\n",
       "For each element in the input sequence, each layer computes the following\n",
       "function:\n",
       "\n",
       ".. math::\n",
       "    \\begin{array}{ll} \\\\\n",
       "        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
       "        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
       "        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
       "        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
       "        c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
       "        h_t = o_t \\odot \\tanh(c_t) \\\\\n",
       "    \\end{array}\n",
       "\n",
       "where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n",
       "state at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{t-1}`\n",
       "is the hidden state of the layer at time `t-1` or the initial hidden\n",
       "state at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`,\n",
       ":math:`o_t` are the input, forget, cell, and output gates, respectively.\n",
       ":math:`\\sigma` is the sigmoid function, and :math:`\\odot` is the Hadamard product.\n",
       "\n",
       "In a multilayer LSTM, the input :math:`x^{(l)}_t` of the :math:`l` -th layer\n",
       "(:math:`l >= 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by\n",
       "dropout :math:`\\delta^{(l-1)}_t` where each :math:`\\delta^{(l-1)}_t` is a Bernoulli random\n",
       "variable which is :math:`0` with probability :attr:`dropout`.\n",
       "\n",
       "If ``proj_size > 0`` is specified, LSTM with projections will be used. This changes\n",
       "the LSTM cell in the following way. First, the dimension of :math:`h_t` will be changed from\n",
       "``hidden_size`` to ``proj_size`` (dimensions of :math:`W_{hi}` will be changed accordingly).\n",
       "Second, the output hidden state of each layer will be multiplied by a learnable projection\n",
       "matrix: :math:`h_t = W_{hr}h_t`. Note that as a consequence of this, the output\n",
       "of LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact\n",
       "dimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.\n",
       "\n",
       "Args:\n",
       "    input_size: The number of expected features in the input `x`\n",
       "    hidden_size: The number of features in the hidden state `h`\n",
       "    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
       "        would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
       "        with the second LSTM taking in outputs of the first LSTM and\n",
       "        computing the final results. Default: 1\n",
       "    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
       "        Default: ``True``\n",
       "    batch_first: If ``True``, then the input and output tensors are provided\n",
       "        as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n",
       "        Note that this does not apply to hidden or cell states. See the\n",
       "        Inputs/Outputs sections below for details.  Default: ``False``\n",
       "    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
       "        LSTM layer except the last layer, with dropout probability equal to\n",
       "        :attr:`dropout`. Default: 0\n",
       "    bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n",
       "    proj_size: If ``> 0``, will use LSTM with projections of corresponding size. Default: 0\n",
       "\n",
       "Inputs: input, (h_0, c_0)\n",
       "    * **input**: tensor of shape :math:`(L, N, H_{in})` when ``batch_first=False`` or\n",
       "      :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n",
       "      the input sequence.  The input can also be a packed variable length sequence.\n",
       "      See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
       "      :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
       "    * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n",
       "      initial hidden state for each element in the batch.\n",
       "      Defaults to zeros if (h_0, c_0) is not provided.\n",
       "    * **c_0**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n",
       "      initial cell state for each element in the batch.\n",
       "      Defaults to zeros if (h_0, c_0) is not provided.\n",
       "\n",
       "    where:\n",
       "\n",
       "    .. math::\n",
       "        \\begin{aligned}\n",
       "            N ={} & \\text{batch size} \\\\\n",
       "            L ={} & \\text{sequence length} \\\\\n",
       "            D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n",
       "            H_{in} ={} & \\text{input\\_size} \\\\\n",
       "            H_{cell} ={} & \\text{hidden\\_size} \\\\\n",
       "            H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\\n",
       "        \\end{aligned}\n",
       "\n",
       "Outputs: output, (h_n, c_n)\n",
       "    * **output**: tensor of shape :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n",
       "      :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n",
       "      `(h_t)` from the last layer of the LSTM, for each `t`. If a\n",
       "      :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n",
       "      will also be a packed sequence.\n",
       "    * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n",
       "      final hidden state for each element in the batch.\n",
       "    * **c_n**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n",
       "      final cell state for each element in the batch.\n",
       "\n",
       "Attributes:\n",
       "    weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n",
       "        `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size, input_size)` for `k = 0`.\n",
       "        Otherwise, the shape is `(4*hidden_size, num_directions * hidden_size)`. If\n",
       "        ``proj_size > 0`` was specified, the shape will be\n",
       "        `(4*hidden_size, num_directions * proj_size)` for `k > 0`\n",
       "    weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n",
       "        `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size, hidden_size)`. If ``proj_size > 0``\n",
       "        was specified, the shape will be `(4*hidden_size, proj_size)`.\n",
       "    bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n",
       "        `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n",
       "    bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n",
       "        `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n",
       "    weight_hr_l[k] : the learnable projection weights of the :math:`\\text{k}^{th}` layer\n",
       "        of shape `(proj_size, hidden_size)`. Only present when ``proj_size > 0`` was\n",
       "        specified.\n",
       "    weight_ih_l[k]_reverse: Analogous to `weight_ih_l[k]` for the reverse direction.\n",
       "        Only present when ``bidirectional=True``.\n",
       "    weight_hh_l[k]_reverse:  Analogous to `weight_hh_l[k]` for the reverse direction.\n",
       "        Only present when ``bidirectional=True``.\n",
       "    bias_ih_l[k]_reverse:  Analogous to `bias_ih_l[k]` for the reverse direction.\n",
       "        Only present when ``bidirectional=True``.\n",
       "    bias_hh_l[k]_reverse:  Analogous to `bias_hh_l[k]` for the reverse direction.\n",
       "        Only present when ``bidirectional=True``.\n",
       "    weight_hr_l[k]_reverse:  Analogous to `weight_hr_l[k]` for the reverse direction.\n",
       "        Only present when ``bidirectional=True`` and ``proj_size > 0`` was specified.\n",
       "\n",
       ".. note::\n",
       "    All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
       "    where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
       "\n",
       ".. note::\n",
       "    For bidirectional LSTMs, forward and backward are directions 0 and 1 respectively.\n",
       "    Example of splitting the output layers when ``batch_first=False``:\n",
       "    ``output.view(seq_len, batch, num_directions, hidden_size)``.\n",
       "\n",
       ".. include:: ../cudnn_rnn_determinism.rst\n",
       "\n",
       ".. include:: ../cudnn_persistent_rnn.rst\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> rnn = nn.LSTM(10, 20, 2)\n",
       "    >>> input = torch.randn(5, 3, 10)\n",
       "    >>> h0 = torch.randn(2, 3, 20)\n",
       "    >>> c0 = torch.randn(2, 3, 20)\n",
       "    >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\admin\\appdata\\roaming\\python\\python36\\site-packages\\torch\\nn\\modules\\rnn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?torch.nn.LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2, 20]), torch.Size([4, 2, 10]), torch.Size([4, 2, 10]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = torch.nn.LSTM(5, 10, 2, bidirectional=False)\n",
    "lstm = torch.nn.LSTM(5, 10, 2, bidirectional=True)\n",
    "#seq长度4，batch_size=2\n",
    "input = torch.randn(4 , 2 , 5)\n",
    "h_0 =torch.randn(2*2 , 2 , 10)\n",
    "c_0 =torch.randn(2*2 , 2 , 10)\n",
    "output1,(hn1,cn1)=lstm(input ,(h_0,c_0))\n",
    "output1.shape,hn1.shape,cn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of LSTM(5, 10, num_layers=2)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight_ih_l0', torch.Size([40, 5])),\n",
       " ('weight_hh_l0', torch.Size([40, 10])),\n",
       " ('bias_ih_l0', torch.Size([40])),\n",
       " ('bias_hh_l0', torch.Size([40])),\n",
       " ('weight_ih_l1', torch.Size([40, 10])),\n",
       " ('weight_hh_l1', torch.Size([40, 10])),\n",
       " ('bias_ih_l1', torch.Size([40])),\n",
       " ('bias_hh_l1', torch.Size([40]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(para[0],para[1].shape) for para in list(lstm.named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 10]) torch.Size([2, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "gru = torch.nn.GRU(5,10,2)\n",
    "input = torch.randn(4,2,5)\n",
    "h_0 = torch.randn(2,2,10)\n",
    "output2,h2= gru(input,h_0)\n",
    "print(output2.shape,h2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6957,  0.6993, -0.3725, -0.2313, -0.6991,  0.4444,  0.5149,\n",
       "            0.6077,  0.8198, -0.8009],\n",
       "          [ 0.4669,  0.8639, -0.3396, -0.1665, -0.7004,  0.4820,  0.2837,\n",
       "           -0.1118,  0.7474, -0.0715]],\n",
       " \n",
       "         [[-0.1502,  0.4686, -0.2533,  0.6203,  0.7747,  0.5299, -0.1359,\n",
       "            0.0369, -0.5976, -0.4017],\n",
       "          [ 0.3929,  0.3828, -0.1115,  0.5295,  0.6983,  0.1508,  0.0953,\n",
       "           -0.5248, -0.3344, -0.4349]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.0837, -0.0961, -0.1969,  0.0425, -0.0880,  0.0380,  0.1046,\n",
       "           -0.0930, -0.0611, -0.0668],\n",
       "          [-0.0377, -0.1121, -0.1301, -0.0113, -0.0489,  0.0365,  0.0101,\n",
       "            0.0299,  0.0221,  0.0672]],\n",
       " \n",
       "         [[ 0.0124,  0.1027,  0.0286,  0.1331, -0.0883,  0.1674, -0.0543,\n",
       "            0.2114, -0.1614,  0.0054],\n",
       "          [ 0.0532,  0.1649,  0.0051,  0.1305, -0.0198,  0.0602, -0.2208,\n",
       "            0.1866, -0.2101, -0.0285]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.0899,  0.0427,  0.0096,  0.3658, -0.3257,  0.0443,  0.0095,\n",
       "            0.1765,  0.2485, -0.1533],\n",
       "          [-0.4764, -0.2781,  0.1559, -0.0539, -0.6618,  0.6395, -0.3939,\n",
       "           -0.2291,  0.4307, -0.3899]],\n",
       " \n",
       "         [[ 0.1972, -0.0337, -0.0552, -0.2048,  0.2567,  0.2547, -0.2908,\n",
       "           -0.1756,  0.1599, -0.2544],\n",
       "          [ 0.0995, -0.0606, -0.1839, -0.1929,  0.2180,  0.1333, -0.1318,\n",
       "           -0.1834,  0.2196, -0.0759]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn0,hn1,h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2196, -0.1863, -0.5052,  0.0954, -0.2868,  0.0638,  0.2701,\n",
       "          -0.1932, -0.1768, -0.1228],\n",
       "         [-0.0795, -0.2140, -0.3298, -0.0235, -0.1308,  0.0796,  0.0219,\n",
       "           0.0893,  0.0603,  0.1613]],\n",
       "\n",
       "        [[ 0.0305,  0.2051,  0.0602,  0.2996, -0.1438,  0.3860, -0.0891,\n",
       "           0.4270, -0.3092,  0.0113],\n",
       "         [ 0.1255,  0.3304,  0.0103,  0.2761, -0.0308,  0.1382, -0.3696,\n",
       "           0.3601, -0.4045, -0.0607]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight_ih_l0', torch.Size([30, 5])),\n",
       " ('weight_hh_l0', torch.Size([30, 10])),\n",
       " ('bias_ih_l0', torch.Size([30])),\n",
       " ('bias_hh_l0', torch.Size([30])),\n",
       " ('weight_ih_l1', torch.Size([30, 10])),\n",
       " ('weight_hh_l1', torch.Size([30, 10])),\n",
       " ('bias_ih_l1', torch.Size([30])),\n",
       " ('bias_hh_l1', torch.Size([30]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(para[0],para[1].shape) for para in list(gru.named_parameters())]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
